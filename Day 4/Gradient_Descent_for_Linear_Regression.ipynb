{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvx6Ar4c1pSe"
      },
      "source": [
        "# Gradient descent for Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlKB5q4D1y10"
      },
      "source": [
        "Now that you have learned how Gradient Descent works, let's use it to find the parameters of Linear Regression. Let's fit the linear regression on the same [Advertisesment Dataset](https://www.statlearning.com/s/Advertising.csv), but this time using Gradient Descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReVUHCNHiGWL"
      },
      "source": [
        "Let's import the necessary libraries first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AcqV3VsCGXpY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnBw7u4u_L_O"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Fx3xywro_Pv3",
        "outputId": "2aebed00-23a5-46a8-bab0-9637c6469715"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TV</th>\n",
              "      <th>radio</th>\n",
              "      <th>newspaper</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>230.1</td>\n",
              "      <td>37.8</td>\n",
              "      <td>69.2</td>\n",
              "      <td>22.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44.5</td>\n",
              "      <td>39.3</td>\n",
              "      <td>45.1</td>\n",
              "      <td>10.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17.2</td>\n",
              "      <td>45.9</td>\n",
              "      <td>69.3</td>\n",
              "      <td>9.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>151.5</td>\n",
              "      <td>41.3</td>\n",
              "      <td>58.5</td>\n",
              "      <td>18.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>180.8</td>\n",
              "      <td>10.8</td>\n",
              "      <td>58.4</td>\n",
              "      <td>12.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      TV  radio  newspaper  sales\n",
              "1  230.1   37.8       69.2   22.1\n",
              "2   44.5   39.3       45.1   10.4\n",
              "3   17.2   45.9       69.3    9.3\n",
              "4  151.5   41.3       58.5   18.5\n",
              "5  180.8   10.8       58.4   12.9"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_path = \"https://www.statlearning.com/s/Advertising.csv\"\n",
        "\n",
        "\n",
        "# Read the CSV data from the link\n",
        "data_df = pd.read_csv(data_path,index_col=0)\n",
        "\n",
        "# Print out first 5 samples from the DataFrame\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce49736IcehF"
      },
      "source": [
        "This is a multiple Linear Regression problem with three independent variables: TV , radio and newspaper and one dependent variable: sales. There are 200 samples in the dataset *ie.* $n = 200$\n",
        "\n",
        "This multiple linear regression problem can be represented in matrix form as:\n",
        "\n",
        "$$\\mathbf{\\hat{y}} = \\mathbf{X} \\boldsymbol{\\beta}$$\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "\\hat{y_1} \\\\\n",
        "\\hat{y_2} \\\\\n",
        "\\vdots \\\\\n",
        "\\hat{y_{200}}\n",
        "\\end{bmatrix} =   \\begin{bmatrix}\n",
        "  1 & x_{1\\ 1} & x_{1\\ 2} & x_{1\\ 3} \\\\\n",
        "  1 & x_{2\\ 1} & x_{2\\ 2} & x_{2\\ 3} \\\\\n",
        "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "  1 & x_{200\\ 1} & x_{200\\ 2} & x_{200\\ 3}\n",
        " \\end{bmatrix} \\times \\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\beta_2 \\\\\n",
        "\\beta_3\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "The predicted output for the samples can be computed as:\n",
        "\\begin{align*}\\hat{y_1} &= \\beta_0x_{1\\ 0}+ \\beta_1x_{1\\ 1} + \\beta_2x_{1\\ 2} + \\beta_3 x_{1\\ 3}\\\\\n",
        "\\hat{y_2} &= \\beta_0x_{2\\ 0}+ \\beta_2x_{2\\ 1} + \\beta_2x_{2\\ 2} + \\beta_3 x_{2\\ 3}\\\\\n",
        "\\hat{y_3} &= \\beta_0x_{3\\ 0}+ \\beta_1x_{3\\ 1} + \\beta_2x_{3\\ 2} + \\beta_3 x_{3\\ 3}\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        ".\\\\\n",
        "\\hat{y_{200}} &= \\beta_0x_{200\\ 0}+ \\beta_1x_{200\\ 1} + \\beta_2x_{200\\ 2} + \\beta_3 x_{200\\ 3}\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Generalizing, for any $i^{th}$ sample, predicted output can be computed as:\n",
        "\n",
        "$$\\hat{y_i} = \\beta_0x_{i\\ 0}+ \\beta_1x_{i\\ 1} + \\beta_2x_{i\\ 2} + \\beta_3 x_{i\\ 3}$$\n",
        " where for all $i$ = $1$ to $n$, $x_{i0} =1$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5BWRU-N_Uz-"
      },
      "source": [
        "The following block of codes creates a matrix $\\mathbf{X}$ containing the features of all the samples and a vector $\\mathbf{y}$ containing their corresponding outputs. It also assigns the number of samples to $n$ and the number of features to $d$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qbsl4vbTFk3i"
      },
      "outputs": [],
      "source": [
        "# Feature matrix\n",
        "X = data_df[['TV', 'radio', 'newspaper']].values\n",
        "\n",
        "# Adding the column of ones in X\n",
        "X = np.c_[np.ones((X.shape[0],1)),X]\n",
        "\n",
        "# Outputs\n",
        "y = data_df['sales'].values.reshape(-1,1)\n",
        "\n",
        "n = X.shape[0] # number of samples (rows)\n",
        "d = X.shape[1] # number of features (columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PucmdL0EtSaM"
      },
      "source": [
        "## Random Initialization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nt6JrDuKidI"
      },
      "source": [
        "Let's initialize the values of parameters randomly. The function `initialize_beta` uses the [`numpy.random.randn`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html) function to initialize the parameters using the random values sampled from **standard normal distribution**. It returns an array of the shape $d\\times 1$ (where $d$ = no. of features) containing the initial values of the parameters. In our case $d=4$ (including the ones column).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abLGKrPft2JS",
        "outputId": "a1b83393-7e6a-4976-d12c-5d45806de863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.76405235]\n",
            " [0.40015721]\n",
            " [0.97873798]\n",
            " [2.2408932 ]]\n"
          ]
        }
      ],
      "source": [
        "def initialize_betas(X, y):\n",
        "  np.random.seed(0)\n",
        "  betas = np.random.randn(d,1)\n",
        "  return betas\n",
        "\n",
        "betas = initialize_betas(X, y)\n",
        "print(betas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKnXrYVtOxI0"
      },
      "source": [
        "Here, the initial values for our parameters are:\n",
        "\n",
        "$$\\boldsymbol{\\beta} =\\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\beta_2 \\\\\n",
        "\\beta_3\n",
        "\\end{bmatrix} =   \\begin{bmatrix}\n",
        "  1.7640\\\\\n",
        "  0.4001\\\\\n",
        "  0.9787\\\\\n",
        "  2.2408\n",
        " \\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOW6cAC9_kBG"
      },
      "source": [
        "## Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhcPRsmcvQRG"
      },
      "source": [
        "In OLS, you minimized the sum of squared error (SSE). Here you will be minimizing the cost function. The cost function $J(.)$ is nothing but the sum of squared error multiplied by $\\frac{1}{2}$ to make the derivation easier. You should know that multiplying the cost function with $\\frac{1}{2}$ only changes the value of the cost function but not the optimal parameters that minimize it.\n",
        "\n",
        "\\begin{align*}\n",
        "J(\\beta_0, \\beta_1, \\beta_2, \\beta_3) &= \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2 \\\\\n",
        "&= \\frac{1}{2}\\sum_{i=1}^{n}((\\beta_0x_{i0}+\\beta_1x_{i1} +\\beta_2x_{i2} + \\beta_3x_{i3})-y_{i})^2\n",
        "\\end{align*}\n",
        "\n",
        "The cost function can be written in matrix form as:\n",
        "\n",
        "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2}\\ \\sum(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})^2$$\n",
        "\n",
        "\n",
        "*Note: We call $J$ as a function of only parameters $\\boldsymbol{\\beta}$ but not of $X$ and $y$ because $X$ and $y$ are constants given by the dataset. So the value of $J$ depends only on the parameters.*\n",
        "\n",
        "**You want to find the parameters $\\beta_0, \\beta_1, \\beta_2$ and $\\beta_3$ that minimizes the cost function $J$ using Gradient Descent.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liAx8tsai2No"
      },
      "source": [
        "The `calculate_cost` function below calculates the cost function for a particular set of values of parameters `betas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4OEEDOSFdco",
        "outputId": "f30a2ad6-4757-47ac-cfe1-796221a4768a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost with random betas: 2303714.226243876\n"
          ]
        }
      ],
      "source": [
        "def calculate_cost(betas):\n",
        "  cost = 1/2 * np.sum(np.square(np.dot(X, betas)-y))\n",
        "  return cost\n",
        "\n",
        "print(\"Cost with random betas:\", calculate_cost(betas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbvmnpbKj0wk"
      },
      "source": [
        "## Gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G07bkOBORc5Y"
      },
      "source": [
        "You will need to calculate the gradient of the cost function with respect to each of the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxNS4h6TobSc"
      },
      "source": [
        "Partial derivative(gradient) of the cost function with respect to $\\beta_1$,\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial J}{\\partial \\beta_1} &= \\frac{\\partial}{\\partial \\beta_1}\\ \\frac{1}{2}\\ \\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2\\\\\n",
        "&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial}{\\partial \\beta_1}(\\hat{y_{i}}-{y_{i}})^2\n",
        "\\end{align*}\n",
        "\n",
        "Applying chain rule,\n",
        "\n",
        "\\begin{align*}\n",
        "\\hspace{8cm}&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial (\\hat{y_{i}}-{y_{i}})^2}{\\partial (\\hat{y_{i}}-{y_{i}})} \\times \\frac{\\partial (\\hat{y_{i}}-{y_{i}})}{\\partial \\beta_1}\\\\\n",
        "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times \\frac{\\partial (\\beta_0x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3})-y_i)}{\\partial \\beta_1}\\\\\n",
        "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times x_{i1}\\\\\n",
        "\\therefore \\frac{\\partial J}{\\partial \\beta_1}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i1}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVtD6EAo_1g"
      },
      "source": [
        "Similarly,\n",
        "\n",
        "\\begin{align*}\\frac{\\partial J}{\\partial \\beta_0}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i0}\\\\\n",
        "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\\times 1\\\\\n",
        "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\beta_2}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i2}$$\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\beta_3}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i3}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thYWZqkPSQ85"
      },
      "source": [
        "In general, the formula for calculating the gradients with respect to a parameter $\\beta_j$ can be expressed as:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\beta_j}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{ij}$$\n",
        "\n",
        "\n",
        "We can write this generalized expression in matrix form to calculate the gradients wrt. all the parameters simultaneously as:\n",
        "\n",
        "$$\\frac{\\boldsymbol{\\partial J}}{\\boldsymbol{\\partial \\beta}}= \\mathbf{X^T}(\\mathbf{\\hat{y}-y}) = \\begin{bmatrix}\n",
        "\\frac{\\partial J}{\\partial \\beta_0} \\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_2}\\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_3}\n",
        "\\end{bmatrix} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Xz0hZVjaMr"
      },
      "source": [
        "The `calculate_gradient` function below calculates the gradients of cost function with respect to the parameters `betas`. It uses the matrix operations to compute the gradient of all the parameters simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGTQFH696hyd",
        "outputId": "86663aab-87c8-4b1f-a297-c672af6466e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradients for random betas = \n",
            " [[  27563.85598559]\n",
            " [4631129.37253468]\n",
            " [ 731917.2683292 ]\n",
            " [1079270.65268036]]\n"
          ]
        }
      ],
      "source": [
        "def calculate_gradients(betas):\n",
        "  gradients = np.dot(X.T,(np.dot(X,betas)-y))\n",
        "  return gradients\n",
        "\n",
        "print(\"Gradients for random betas = \\n\", calculate_gradients(betas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJigUKetTBqB"
      },
      "source": [
        "Here, the gradients of the cost function with respect to the initial parameters are:\n",
        "\n",
        "$$\\boldsymbol{\\frac{\\partial J}{\\partial \\beta}}  =\\begin{bmatrix}\n",
        "\\frac{\\partial J}{\\partial \\beta_0} \\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_2}\\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_3}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "27563.85 \\\\\n",
        "4631129.37\\\\\n",
        "731917.26\\\\\n",
        "1079270.65\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_wG80YPmuZG"
      },
      "source": [
        "You can see that the gradients for different parameters vary quite largely. The parameters corresponding to the features having larger values have larger gradients and the parameters corresponding to the features having smaller values have smaller gradients. For example, the gradients with respect to the parameter corresponding to the feature 'TV', $\\frac{\\partial J}{\\partial \\beta_1}$ is larger than the ones corresponing to the 'radio', $\\frac{\\partial J}{\\partial \\beta_2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcyFQ69asVOs"
      },
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tVDczBZUNUw"
      },
      "source": [
        "Now you need to update the parameters using their respective gradients until the cost function converges to its minimum value.\n",
        "\n",
        "\n",
        "${\\hspace{5cm}}\\text{Repeat until convergence }\\{$\n",
        "\n",
        "$$\\beta_0 :=\\beta_0-\\alpha\\frac{\\partial J}{\\partial \\beta_0}$$\n",
        "\n",
        "$$\\beta_1 :=\\beta_1-\\alpha\\frac{\\partial J}{\\partial \\beta_1}$$\n",
        "\n",
        "$$\\beta_2 :=\\beta_2-\\alpha\\frac{\\partial J}{\\partial \\beta_2}$$\n",
        "\n",
        "$$\\beta_3 :=\\beta_3-\\alpha\\frac{\\partial J}{\\partial \\beta_3}$$\n",
        "\n",
        "${\\hspace{8cm}}\\}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Since you already have a vector $\\beta$ called `beta` containing parameters and an another vector $\\frac{\\partial J}{\\partial \\beta}$ called `gradients` containing the gradients of cost function with respect to the parameters, this updation is a simple matrix operation:\n",
        "\n",
        "$$\\boldsymbol{\\beta} := \\boldsymbol{\\beta} - \\alpha \\boldsymbol{\\frac{\\partial J}{\\partial \\beta}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adoBYUlCk38k"
      },
      "source": [
        "The `gradient_descent` function below applies the gradient descent algorithm to find the optimal parameters `betas` that minimize the cost function. Initially `betas` contain the random initial values of the parameters. It uses the gradients calculated by the `calculate_gradients` function to update the values of the parameters. The default learning rate is set to `alpha=0.003`. The process of updating the parameters is repeated till the cost function decreases by a certain threshold value `precision` or the maximum number of iterations `max_iters` is not reached. Also the list `costs`  contains the values of cost functions for different values of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZVha43ihsdqB"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, alpha=0.003 , max_iters=10000, precision = 1e-3):\n",
        "  iteration = 0 # no. of iterations\n",
        "  difference = 1\n",
        "  betas = initialize_betas(X,y) # random initial values of the parameters\n",
        "  cost = calculate_cost(betas) # cost for the initial values pf parameters\n",
        "  costs = [calculate_cost(betas)] # list containing the history of costs for different iterations\n",
        "\n",
        "  while difference > precision and iteration <= max_iters :\n",
        "    # updating the values of parameters\n",
        "    betas = betas - alpha * calculate_gradients(betas)\n",
        "\n",
        "    # cost for the new values of parameters\n",
        "    cost = calculate_cost(betas)\n",
        "\n",
        "    # difference between the cost of current iteration and previous iteration\n",
        "    difference = np.abs(costs[iteration] - cost)\n",
        "    costs.append(cost)\n",
        "\n",
        "    print(\"iteration: {}, cost: {}\".format(iteration, cost))\n",
        "    iteration += 1\n",
        "\n",
        "    if(cost == np.infty):\n",
        "      print(\"Cost reached infinity, try smaller learning rate\")\n",
        "      break\n",
        "\n",
        "  return betas, iteration, costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQhroBc1rvzi"
      },
      "source": [
        "Let's use the `gradient_descent` function defined above to learn the parameters for our multiple linear regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7rZkEHg0jga",
        "outputId": "c5c8da0b-96dc-4187-9b3a-5421df6036b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 0, cost: 624562462609042.5\n",
            "iteration: 1, cost: 2.04188800865341e+23\n",
            "iteration: 2, cost: 6.6765064007188725e+31\n",
            "iteration: 3, cost: 2.1830649810113783e+40\n",
            "iteration: 4, cost: 7.138123481955464e+48\n",
            "iteration: 5, cost: 2.3340032150596278e+57\n",
            "iteration: 6, cost: 7.631657005765796e+65\n",
            "iteration: 7, cost: 2.49537739613466e+74\n",
            "iteration: 8, cost: 8.159313690900017e+82\n",
            "iteration: 9, cost: 2.6679090709738826e+91\n",
            "iteration: 10, cost: 8.723452830258335e+99\n",
            "iteration: 11, cost: 2.852369674426849e+108\n",
            "iteration: 12, cost: 9.326596839463841e+116\n",
            "iteration: 13, cost: 3.0495839787448193e+125\n",
            "iteration: 14, cost: 9.971442535251373e+133\n",
            "iteration: 15, cost: 3.2604337813561245e+142\n",
            "iteration: 16, cost: 1.0660873193649929e+151\n",
            "iteration: 17, cost: 3.4858618476162103e+159\n",
            "iteration: 18, cost: 1.1397971441874089e+168\n",
            "iteration: 19, cost: 3.726876126161402e+176\n",
            "iteration: 20, cost: 1.2186033041567317e+185\n",
            "iteration: 21, cost: 3.984554255714459e+193\n",
            "iteration: 22, cost: 1.3028581625025874e+202\n",
            "iteration: 23, cost: 4.2600483834929084e+210\n",
            "iteration: 24, cost: 1.3929384450292768e+219\n",
            "iteration: 25, cost: 4.554590316764671e+227\n",
            "iteration: 26, cost: 1.4892469245567063e+236\n",
            "iteration: 27, cost: 4.86949703058485e+244\n",
            "iteration: 28, cost: 1.592214221824421e+253\n",
            "iteration: 29, cost: 5.206176556340283e+261\n",
            "iteration: 30, cost: 1.7023007309110713e+270\n",
            "iteration: 31, cost: 5.566134277431072e+278\n",
            "iteration: 32, cost: 1.8199986777783747e+287\n",
            "iteration: 33, cost: 5.950979660238805e+295\n",
            "iteration: 34, cost: 1.945834321143854e+304\n",
            "iteration: 35, cost: inf\n",
            "Cost reached infinity, try smaller learning rate\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_22964\\3834816075.py:2: RuntimeWarning: overflow encountered in square\n",
            "  cost = 1/2 * np.sum(np.square(np.dot(X, betas)-y))\n"
          ]
        }
      ],
      "source": [
        "betas, steps, costs = gradient_descent(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pCaStNn22KA"
      },
      "source": [
        "As you can see the default learning rate of `alpha=0.003` seems to be very large for this problem. The reason behind this is that the value of the gradients are very large. When the parameters are updated using these gradients, the parameters too get large values. After some iterations the cost function calculated using these parameters reach infinity.\n",
        "\n",
        "A possible solution to this is scaling the features to small values about which we will learn  in the next chapter. For now, let's further decrease the value of learning rate to `alpha=0.0000003`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSrmNc5s3xPJ",
        "outputId": "eec0b3df-74fa-4fb2-ad6c-46ff1346bc94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 0, cost: 1605439.1704912465\n",
            "iteration: 1, cost: 1139934.3901561839\n",
            "iteration: 2, cost: 827385.9388970847\n",
            "iteration: 3, cost: 615561.209976966\n",
            "iteration: 4, cost: 470257.5167967307\n",
            "iteration: 5, cost: 369060.20773126744\n",
            "iteration: 6, cost: 297263.3981504862\n",
            "iteration: 7, cost: 245203.88220357374\n",
            "iteration: 8, cost: 206518.51314390055\n",
            "iteration: 9, cost: 177005.05468340195\n",
            "iteration: 10, cost: 153877.40255986893\n",
            "iteration: 11, cost: 135278.53927748697\n",
            "iteration: 12, cost: 119961.93545091985\n",
            "iteration: 13, cost: 107083.05179937743\n",
            "iteration: 14, cost: 96062.81371070776\n",
            "iteration: 15, cost: 86498.14156564066\n",
            "iteration: 16, cost: 78103.25292282502\n",
            "iteration: 17, cost: 70671.09390908064\n",
            "iteration: 18, cost: 64047.943533163925\n",
            "iteration: 19, cost: 58116.64362275175\n",
            "iteration: 20, cost: 52785.481359982216\n",
            "iteration: 21, cost: 47980.78022772065\n",
            "iteration: 22, cost: 43641.92759428901\n",
            "iteration: 23, cost: 39718.00667002998\n",
            "iteration: 24, cost: 36165.48787214866\n",
            "iteration: 25, cost: 32946.62247261574\n",
            "iteration: 26, cost: 30028.30423907675\n",
            "iteration: 27, cost: 27381.245129862684\n",
            "iteration: 28, cost: 24979.363686890007\n",
            "iteration: 29, cost: 22799.319200830403\n",
            "iteration: 30, cost: 20820.14728572889\n",
            "iteration: 31, cost: 19022.96730223446\n",
            "iteration: 32, cost: 17390.74179397066\n",
            "iteration: 33, cost: 15908.074504756427\n",
            "iteration: 34, cost: 14561.037772258165\n",
            "iteration: 35, cost: 13337.022896030652\n",
            "iteration: 36, cost: 12224.608945120275\n",
            "iteration: 37, cost: 11213.446723202855\n",
            "iteration: 38, cost: 10294.155457505522\n",
            "iteration: 39, cost: 9458.230358953264\n",
            "iteration: 40, cost: 8697.959605147129\n",
            "iteration: 41, cost: 8006.349584004234\n",
            "iteration: 42, cost: 7377.057442986668\n",
            "iteration: 43, cost: 6804.330142372715\n",
            "iteration: 44, cost: 6282.949327852182\n",
            "iteration: 45, cost: 5808.181429040491\n",
            "iteration: 46, cost: 5375.732463753646\n",
            "iteration: 47, cost: 4981.707088065561\n",
            "iteration: 48, cost: 4622.571482663992\n",
            "iteration: 49, cost: 4295.119709148256\n",
            "iteration: 50, cost: 3996.443207282262\n",
            "iteration: 51, cost: 3723.9031369700015\n",
            "iteration: 52, cost: 3475.1052976825413\n",
            "iteration: 53, cost: 3247.877383846181\n",
            "iteration: 54, cost: 3040.24835776596\n",
            "iteration: 55, cost: 2850.4297423695743\n",
            "iteration: 56, cost: 2676.798654705298\n",
            "iteration: 57, cost: 2517.8824179524026\n",
            "iteration: 58, cost: 2372.3446049044333\n",
            "iteration: 59, cost: 2238.9723796353983\n",
            "iteration: 60, cost: 2116.6650165048686\n",
            "iteration: 61, cost: 2004.4234869299319\n",
            "iteration: 62, cost: 1901.3410145647194\n",
            "iteration: 63, cost: 1806.594508784064\n",
            "iteration: 64, cost: 1719.4367947581536\n",
            "iteration: 65, cost: 1639.1895660118455\n",
            "iteration: 66, cost: 1565.2369922596047\n",
            "iteration: 67, cost: 1497.0199215614025\n",
            "iteration: 68, cost: 1434.0306215166813\n",
            "iteration: 69, cost: 1375.808009357069\n",
            "iteration: 70, cost: 1321.9333254633057\n",
            "iteration: 71, cost: 1272.0262090624205\n",
            "iteration: 72, cost: 1225.7411386980984\n",
            "iteration: 73, cost: 1182.7642035470358\n",
            "iteration: 74, cost: 1142.8101748101637\n",
            "iteration: 75, cost: 1105.6198492700817\n",
            "iteration: 76, cost: 1070.9576397021688\n",
            "iteration: 77, cost: 1038.6093891814758\n",
            "iteration: 78, cost: 1008.3803884630565\n",
            "iteration: 79, cost: 980.0935775503142\n",
            "iteration: 80, cost: 953.5879143226368\n",
            "iteration: 81, cost: 928.7168946869094\n",
            "iteration: 82, cost: 905.3472101625639\n",
            "iteration: 83, cost: 883.3575301204694\n",
            "iteration: 84, cost: 862.6373970847117\n",
            "iteration: 85, cost: 843.0862245844401\n",
            "iteration: 86, cost: 824.6123880208254\n",
            "iteration: 87, cost: 807.1323999010551\n",
            "iteration: 88, cost: 790.5701615956875\n",
            "iteration: 89, cost: 774.8562845052422\n",
            "iteration: 90, cost: 759.9274741836043\n",
            "iteration: 91, cost: 745.7259715659609\n",
            "iteration: 92, cost: 732.199045993306\n",
            "iteration: 93, cost: 719.2985352192298\n",
            "iteration: 94, cost: 706.9804280324776\n",
            "iteration: 95, cost: 695.2044855348566\n",
            "iteration: 96, cost: 683.9338974824182\n",
            "iteration: 97, cost: 673.1349704318984\n",
            "iteration: 98, cost: 662.776844737404\n",
            "iteration: 99, cost: 652.8312377171359\n",
            "iteration: 100, cost: 643.2722105591881\n",
            "iteration: 101, cost: 634.075956761532\n",
            "iteration: 102, cost: 625.2206101063141\n",
            "iteration: 103, cost: 616.6860703545809\n",
            "iteration: 104, cost: 608.4538450161833\n",
            "iteration: 105, cost: 600.5069057026174\n",
            "iteration: 106, cost: 592.8295577092799\n",
            "iteration: 107, cost: 585.4073215994817\n",
            "iteration: 108, cost: 578.2268256766804\n",
            "iteration: 109, cost: 571.2757083349279\n",
            "iteration: 110, cost: 564.5425293714143\n",
            "iteration: 111, cost: 558.0166894301481\n",
            "iteration: 112, cost: 551.6883568230573\n",
            "iteration: 113, cost: 545.5484010448431\n",
            "iteration: 114, cost: 539.588332361462\n",
            "iteration: 115, cost: 533.8002469097431\n",
            "iteration: 116, cost: 528.1767767979119\n",
            "iteration: 117, cost: 522.7110447442064\n",
            "iteration: 118, cost: 517.3966228337684\n",
            "iteration: 119, cost: 512.227495012994\n",
            "iteration: 120, cost: 507.19802297590036\n",
            "iteration: 121, cost: 502.30291512915613\n",
            "iteration: 122, cost: 497.53719835151423\n",
            "iteration: 123, cost: 492.8961922897877\n",
            "iteration: 124, cost: 488.3754859574501\n",
            "iteration: 125, cost: 483.970916423651\n",
            "iteration: 126, cost: 479.67854940013865\n",
            "iteration: 127, cost: 475.4946615514434\n",
            "iteration: 128, cost: 471.41572436987764\n",
            "iteration: 129, cost: 467.4383894716092\n",
            "iteration: 130, cost: 463.55947518339315\n",
            "iteration: 131, cost: 459.7759543016391\n",
            "iteration: 132, cost: 456.08494291645763\n",
            "iteration: 133, cost: 452.4836902032833\n",
            "iteration: 134, cost: 448.96956909368566\n",
            "iteration: 135, cost: 445.5400677451778\n",
            "iteration: 136, cost: 442.19278173724234\n",
            "iteration: 137, cost: 438.92540692754085\n",
            "iteration: 138, cost: 435.73573290837714\n",
            "iteration: 139, cost: 432.62163700902454\n",
            "iteration: 140, cost: 429.58107879455815\n",
            "iteration: 141, cost: 426.61209501638973\n",
            "iteration: 142, cost: 423.71279497384\n",
            "iteration: 143, cost: 420.8813562498376\n",
            "iteration: 144, cost: 418.11602078723354\n",
            "iteration: 145, cost: 415.4150912753128\n",
            "iteration: 146, cost: 412.7769278188824\n",
            "iteration: 147, cost: 410.19994486485814\n",
            "iteration: 148, cost: 407.68260836357786\n",
            "iteration: 149, cost: 405.2234331441617\n",
            "iteration: 150, cost: 402.82098048513365\n",
            "iteration: 151, cost: 400.473855863248\n",
            "iteration: 152, cost: 398.1807068650161\n",
            "iteration: 153, cost: 395.94022124685785\n",
            "iteration: 154, cost: 393.7511251310782\n",
            "iteration: 155, cost: 391.6121813260438\n",
            "iteration: 156, cost: 389.522187759988\n",
            "iteration: 157, cost: 387.47997601883725\n",
            "iteration: 158, cost: 385.4844099793243\n",
            "iteration: 159, cost: 383.53438452943936\n",
            "iteration: 160, cost: 381.62882436899747\n",
            "iteration: 161, cost: 379.76668288374503\n",
            "iteration: 162, cost: 377.94694108702356\n",
            "iteration: 163, cost: 376.16860662354543\n",
            "iteration: 164, cost: 374.4307128303227\n",
            "iteration: 165, cost: 372.73231785023654\n",
            "iteration: 166, cost: 371.07250379413017\n",
            "iteration: 167, cost: 369.450375947681\n",
            "iteration: 168, cost: 367.865062019632\n",
            "iteration: 169, cost: 366.3157114282684\n",
            "iteration: 170, cost: 364.8014946232964\n",
            "iteration: 171, cost: 363.32160244052966\n",
            "iteration: 172, cost: 361.8752454870147\n",
            "iteration: 173, cost: 360.4616535544325\n",
            "iteration: 174, cost: 359.08007505879664\n",
            "iteration: 175, cost: 357.72977650464213\n",
            "iteration: 176, cost: 356.4100419720487\n",
            "iteration: 177, cost: 355.1201726249843\n",
            "iteration: 178, cost: 353.8594862395821\n",
            "iteration: 179, cost: 352.6273167510772\n",
            "iteration: 180, cost: 351.4230138182389\n",
            "iteration: 181, cost: 350.24594240422505\n",
            "iteration: 182, cost: 349.0954823728764\n",
            "iteration: 183, cost: 347.97102809954515\n",
            "iteration: 184, cost: 346.87198809562494\n",
            "iteration: 185, cost: 345.7977846460175\n",
            "iteration: 186, cost: 344.7478534588255\n",
            "iteration: 187, cost: 343.7216433266236\n",
            "iteration: 188, cost: 342.7186157987006\n",
            "iteration: 189, cost: 341.73824486371893\n",
            "iteration: 190, cost: 340.78001664227327\n",
            "iteration: 191, cost: 339.8434290888721\n",
            "iteration: 192, cost: 338.92799170289646\n",
            "iteration: 193, cost: 338.03322524812575\n",
            "iteration: 194, cost: 337.15866148044563\n",
            "iteration: 195, cost: 336.3038428833819\n",
            "iteration: 196, cost: 335.46832241112713\n",
            "iteration: 197, cost: 334.65166323874973\n",
            "iteration: 198, cost: 333.8534385192932\n",
            "iteration: 199, cost: 333.07323114749533\n",
            "iteration: 200, cost: 332.31063352987155\n",
            "iteration: 201, cost: 331.5652473609215\n",
            "iteration: 202, cost: 330.836683405236\n",
            "iteration: 203, cost: 330.1245612852905\n",
            "iteration: 204, cost: 329.4285092747255\n",
            "iteration: 205, cost: 328.74816409692573\n",
            "iteration: 206, cost: 328.0831707287199\n",
            "iteration: 207, cost: 327.43318220902944\n",
            "iteration: 208, cost: 326.7978594523105\n",
            "iteration: 209, cost: 326.176871066633\n",
            "iteration: 210, cost: 325.5698931762547\n",
            "iteration: 211, cost: 324.9766092485517\n",
            "iteration: 212, cost: 324.39670992517455\n",
            "iteration: 213, cost: 323.829892857305\n",
            "iteration: 214, cost: 323.27586254489336\n",
            "iteration: 215, cost: 322.73433017976254\n",
            "iteration: 216, cost: 322.2050134924702\n",
            "iteration: 217, cost: 321.68763660282207\n",
            "iteration: 218, cost: 321.18192987393707\n",
            "iteration: 219, cost: 320.68762976976893\n",
            "iteration: 220, cost: 320.204478715987\n",
            "iteration: 221, cost: 319.7322249641322\n",
            "iteration: 222, cost: 319.2706224589573\n",
            "iteration: 223, cost: 318.8194307088724\n",
            "iteration: 224, cost: 318.37841465941085\n",
            "iteration: 225, cost: 317.94734456964267\n",
            "iteration: 226, cost: 317.52599589145814\n",
            "iteration: 227, cost: 317.11414915164937\n",
            "iteration: 228, cost: 316.71158983672063\n",
            "iteration: 229, cost: 316.3181082803588\n",
            "iteration: 230, cost: 315.93349955349936\n",
            "iteration: 231, cost: 315.55756335692234\n",
            "iteration: 232, cost: 315.1901039163186\n",
            "iteration: 233, cost: 314.8309298797652\n",
            "iteration: 234, cost: 314.47985421755175\n",
            "iteration: 235, cost: 314.13669412430175\n",
            "iteration: 236, cost: 313.80127092333345\n",
            "iteration: 237, cost: 313.47340997320697\n",
            "iteration: 238, cost: 313.1529405764061\n",
            "iteration: 239, cost: 312.83969589010394\n",
            "iteration: 240, cost: 312.53351283896274\n",
            "iteration: 241, cost: 312.2342320299211\n",
            "iteration: 242, cost: 311.9416976689215\n",
            "iteration: 243, cost: 311.65575747953244\n",
            "iteration: 244, cost: 311.376262623421\n",
            "iteration: 245, cost: 311.10306762263406\n",
            "iteration: 246, cost: 310.8360302836436\n",
            "iteration: 247, cost: 310.57501162311826\n",
            "iteration: 248, cost: 310.31987579537855\n",
            "iteration: 249, cost: 310.0704900214985\n",
            "iteration: 250, cost: 309.82672452001515\n",
            "iteration: 251, cost: 309.5884524392089\n",
            "iteration: 252, cost: 309.3555497909189\n",
            "iteration: 253, cost: 309.1278953858573\n",
            "iteration: 254, cost: 308.90537077039005\n",
            "iteration: 255, cost: 308.68786016474763\n",
            "iteration: 256, cost: 308.4752504026362\n",
            "iteration: 257, cost: 308.2674308722145\n",
            "iteration: 258, cost: 308.06429345840723\n",
            "iteration: 259, cost: 307.8657324865229\n",
            "iteration: 260, cost: 307.67164466714786\n",
            "iteration: 261, cost: 307.48192904228614\n",
            "iteration: 262, cost: 307.2964869327176\n",
            "iteration: 263, cost: 307.11522188654607\n",
            "iteration: 264, cost: 306.9380396289114\n",
            "iteration: 265, cost: 306.764848012837\n",
            "iteration: 266, cost: 306.5955569711899\n",
            "iteration: 267, cost: 306.43007846972534\n",
            "iteration: 268, cost: 306.268326461193\n",
            "iteration: 269, cost: 306.11021684048\n",
            "iteration: 270, cost: 305.9556674007677\n",
            "iteration: 271, cost: 305.8045977906787\n",
            "iteration: 272, cost: 305.6569294723922\n",
            "iteration: 273, cost: 305.5125856807049\n",
            "iteration: 274, cost: 305.37149138301766\n",
            "iteration: 275, cost: 305.2335732402249\n",
            "iteration: 276, cost: 305.09875956848776\n",
            "iteration: 277, cost: 304.9669803018707\n",
            "iteration: 278, cost: 304.8381669558213\n",
            "iteration: 279, cost: 304.71225259147565\n",
            "iteration: 280, cost: 304.589171780769\n",
            "iteration: 281, cost: 304.4688605723344\n",
            "iteration: 282, cost: 304.3512564581716\n",
            "iteration: 283, cost: 304.2362983410682\n",
            "iteration: 284, cost: 304.1239265027564\n",
            "iteration: 285, cost: 304.01408257278877\n",
            "iteration: 286, cost: 303.90670949811727\n",
            "iteration: 287, cost: 303.8017515133574\n",
            "iteration: 288, cost: 303.69915411172565\n",
            "iteration: 289, cost: 303.59886401663175\n",
            "iteration: 290, cost: 303.50082915391283\n",
            "iteration: 291, cost: 303.404998624694\n",
            "iteration: 292, cost: 303.3113226788612\n",
            "iteration: 293, cost: 303.21975268913485\n",
            "iteration: 294, cost: 303.13024112572566\n",
            "iteration: 295, cost: 303.0427415315651\n",
            "iteration: 296, cost: 302.9572084980939\n",
            "iteration: 297, cost: 302.873597641597\n",
            "iteration: 298, cost: 302.79186558007325\n",
            "iteration: 299, cost: 302.7119699106263\n",
            "iteration: 300, cost: 302.63386918736694\n",
            "iteration: 301, cost: 302.5575228998134\n",
            "iteration: 302, cost: 302.4828914517794\n",
            "iteration: 303, cost: 302.4099361407385\n",
            "iteration: 304, cost: 302.3386191376553\n",
            "iteration: 305, cost: 302.2689034672703\n",
            "iteration: 306, cost: 302.2007529888308\n",
            "iteration: 307, cost: 302.13413237725615\n",
            "iteration: 308, cost: 302.06900710472877\n",
            "iteration: 309, cost: 302.0053434227003\n",
            "iteration: 310, cost: 301.94310834430337\n",
            "iteration: 311, cost: 301.88226962716135\n",
            "iteration: 312, cost: 301.8227957565852\n",
            "iteration: 313, cost: 301.7646559291503\n",
            "iteration: 314, cost: 301.7078200366434\n",
            "iteration: 315, cost: 301.652258650372\n",
            "iteration: 316, cost: 301.5979430058282\n",
            "iteration: 317, cost: 301.54484498769835\n",
            "iteration: 318, cost: 301.4929371152111\n",
            "iteration: 319, cost: 301.4421925278161\n",
            "iteration: 320, cost: 301.3925849711862\n",
            "iteration: 321, cost: 301.3440887835351\n",
            "iteration: 322, cost: 301.29667888224435\n",
            "iteration: 323, cost: 301.2503307507918\n",
            "iteration: 324, cost: 301.20502042597525\n",
            "iteration: 325, cost: 301.160724485424\n",
            "iteration: 326, cost: 301.1174200353935\n",
            "iteration: 327, cost: 301.0750846988334\n",
            "iteration: 328, cost: 301.03369660372755\n",
            "iteration: 329, cost: 300.99323437169426\n",
            "iteration: 330, cost: 300.9536771068466\n",
            "iteration: 331, cost: 300.91500438490186\n",
            "iteration: 332, cost: 300.8771962425385\n",
            "iteration: 333, cost: 300.8402331669917\n",
            "iteration: 334, cost: 300.80409608588593\n",
            "iteration: 335, cost: 300.76876635729445\n",
            "iteration: 336, cost: 300.73422576002486\n",
            "iteration: 337, cost: 300.70045648412355\n",
            "iteration: 338, cost: 300.6674411215941\n",
            "iteration: 339, cost: 300.6351626573258\n",
            "iteration: 340, cost: 300.60360446022565\n",
            "iteration: 341, cost: 300.57275027455245\n",
            "iteration: 342, cost: 300.5425842114447\n",
            "iteration: 343, cost: 300.51309074064045\n",
            "iteration: 344, cost: 300.4842546823843\n",
            "iteration: 345, cost: 300.4560611995163\n",
            "iteration: 346, cost: 300.42849578973994\n",
            "iteration: 347, cost: 300.40154427806505\n",
            "iteration: 348, cost: 300.3751928094199\n",
            "iteration: 349, cost: 300.34942784143124\n",
            "iteration: 350, cost: 300.3242361373673\n",
            "iteration: 351, cost: 300.29960475923923\n",
            "iteration: 352, cost: 300.2755210610586\n",
            "iteration: 353, cost: 300.2519726822479\n",
            "iteration: 354, cost: 300.22894754119795\n",
            "iteration: 355, cost: 300.20643382897254\n",
            "iteration: 356, cost: 300.1844200031546\n",
            "iteration: 357, cost: 300.16289478183023\n",
            "iteration: 358, cost: 300.14184713771\n",
            "iteration: 359, cost: 300.12126629238196\n",
            "iteration: 360, cost: 300.10114171069495\n",
            "iteration: 361, cost: 300.081463095268\n",
            "iteration: 362, cost: 300.0622203811246\n",
            "iteration: 363, cost: 300.0434037304466\n",
            "iteration: 364, cost: 300.02500352744835\n",
            "iteration: 365, cost: 300.00701037336535\n",
            "iteration: 366, cost: 299.9894150815554\n",
            "iteration: 367, cost: 299.97220867271284\n",
            "iteration: 368, cost: 299.9553823701873\n",
            "iteration: 369, cost: 299.9389275954113\n",
            "iteration: 370, cost: 299.92283596342884\n",
            "iteration: 371, cost: 299.9070992785262\n",
            "iteration: 372, cost: 299.8917095299606\n",
            "iteration: 373, cost: 299.8766588877858\n",
            "iteration: 374, cost: 299.8619396987714\n",
            "iteration: 375, cost: 299.84754448241455\n",
            "iteration: 376, cost: 299.83346592704197\n",
            "iteration: 377, cost: 299.8196968859989\n",
            "iteration: 378, cost: 299.80623037392553\n",
            "iteration: 379, cost: 299.7930595631163\n",
            "iteration: 380, cost: 299.7801777799615\n",
            "iteration: 381, cost: 299.76757850147\n",
            "iteration: 382, cost: 299.75525535186955\n",
            "iteration: 383, cost: 299.74320209928425\n",
            "iteration: 384, cost: 299.73141265248705\n",
            "iteration: 385, cost: 299.71988105772505\n",
            "iteration: 386, cost: 299.70860149561724\n",
            "iteration: 387, cost: 299.69756827812137\n",
            "iteration: 388, cost: 299.6867758455704\n",
            "iteration: 389, cost: 299.6762187637746\n",
            "iteration: 390, cost: 299.6658917211895\n",
            "iteration: 391, cost: 299.6557895261485\n",
            "iteration: 392, cost: 299.6459071041568\n",
            "iteration: 393, cost: 299.6362394952473\n",
            "iteration: 394, cost: 299.62678185139544\n",
            "iteration: 395, cost: 299.6175294339929\n",
            "iteration: 396, cost: 299.6084776113782\n",
            "iteration: 397, cost: 299.599621856423\n",
            "iteration: 398, cost: 299.59095774417284\n",
            "iteration: 399, cost: 299.5824809495409\n",
            "iteration: 400, cost: 299.5741872450545\n",
            "iteration: 401, cost: 299.5660724986518\n",
            "iteration: 402, cost: 299.55813267152837\n",
            "iteration: 403, cost: 299.5503638160327\n",
            "iteration: 404, cost: 299.5427620736085\n",
            "iteration: 405, cost: 299.53532367278456\n",
            "iteration: 406, cost: 299.5280449272086\n",
            "iteration: 407, cost: 299.5209222337266\n",
            "iteration: 408, cost: 299.51395207050507\n",
            "iteration: 409, cost: 299.50713099519555\n",
            "iteration: 410, cost: 299.50045564314087\n",
            "iteration: 411, cost: 299.4939227256215\n",
            "iteration: 412, cost: 299.4875290281419\n",
            "iteration: 413, cost: 299.4812714087552\n",
            "iteration: 414, cost: 299.475146796426\n",
            "iteration: 415, cost: 299.4691521894297\n",
            "iteration: 416, cost: 299.4632846537884\n",
            "iteration: 417, cost: 299.4575413217419\n",
            "iteration: 418, cost: 299.4519193902531\n",
            "iteration: 419, cost: 299.4464161195476\n",
            "iteration: 420, cost: 299.4410288316852\n",
            "iteration: 421, cost: 299.4357549091649\n",
            "iteration: 422, cost: 299.43059179356095\n",
            "iteration: 423, cost: 299.4255369841892\n",
            "iteration: 424, cost: 299.4205880368038\n",
            "iteration: 425, cost: 299.4157425623241\n",
            "iteration: 426, cost: 299.4109982255889\n",
            "iteration: 427, cost: 299.40635274414\n",
            "iteration: 428, cost: 299.40180388703226\n",
            "iteration: 429, cost: 299.3973494736716\n",
            "iteration: 430, cost: 299.39298737267825\n",
            "iteration: 431, cost: 299.38871550077585\n",
            "iteration: 432, cost: 299.38453182170605\n",
            "iteration: 433, cost: 299.3804343451676\n",
            "iteration: 434, cost: 299.3764211257785\n",
            "iteration: 435, cost: 299.37249026206274\n",
            "iteration: 436, cost: 299.3686398954592\n",
            "iteration: 437, cost: 299.3648682093527\n",
            "iteration: 438, cost: 299.3611734281284\n",
            "iteration: 439, cost: 299.3575538162449\n",
            "iteration: 440, cost: 299.3540076773314\n",
            "iteration: 441, cost: 299.3505333533026\n",
            "iteration: 442, cost: 299.34712922349456\n",
            "iteration: 443, cost: 299.3437937038211\n",
            "iteration: 444, cost: 299.3405252459475\n",
            "iteration: 445, cost: 299.33732233648345\n",
            "iteration: 446, cost: 299.334183496195\n",
            "iteration: 447, cost: 299.3311072792334\n",
            "iteration: 448, cost: 299.32809227238147\n",
            "iteration: 449, cost: 299.3251370943175\n",
            "iteration: 450, cost: 299.3222403948947\n",
            "iteration: 451, cost: 299.31940085443864\n",
            "iteration: 452, cost: 299.3166171830586\n",
            "iteration: 453, cost: 299.3138881199758\n",
            "iteration: 454, cost: 299.31121243286617\n",
            "iteration: 455, cost: 299.30858891721846\n",
            "iteration: 456, cost: 299.30601639570614\n",
            "iteration: 457, cost: 299.3034937175742\n",
            "iteration: 458, cost: 299.30101975803916\n",
            "iteration: 459, cost: 299.29859341770316\n",
            "iteration: 460, cost: 299.2962136219808\n",
            "iteration: 461, cost: 299.2938793205394\n",
            "iteration: 462, cost: 299.2915894867514\n",
            "iteration: 463, cost: 299.28934311715943\n",
            "iteration: 464, cost: 299.2871392309533\n",
            "iteration: 465, cost: 299.2849768694593\n",
            "iteration: 466, cost: 299.28285509563983\n",
            "iteration: 467, cost: 299.28077299360586\n",
            "iteration: 468, cost: 299.27872966813914\n",
            "iteration: 469, cost: 299.27672424422576\n",
            "iteration: 470, cost: 299.2747558666003\n",
            "iteration: 471, cost: 299.2728236992999\n",
            "iteration: 472, cost: 299.27092692522893\n",
            "iteration: 473, cost: 299.269064745733\n",
            "iteration: 474, cost: 299.26723638018234\n",
            "iteration: 475, cost: 299.265441065566\n",
            "iteration: 476, cost: 299.2636780560932\n",
            "iteration: 477, cost: 299.2619466228051\n",
            "iteration: 478, cost: 299.26024605319515\n",
            "iteration: 479, cost: 299.25857565083743\n",
            "iteration: 480, cost: 299.2569347350236\n",
            "iteration: 481, cost: 299.2553226404086\n",
            "iteration: 482, cost: 299.2537387166638\n",
            "iteration: 483, cost: 299.2521823281378\n",
            "iteration: 484, cost: 299.2506528535254\n",
            "iteration: 485, cost: 299.24914968554384\n",
            "iteration: 486, cost: 299.2476722306162\n",
            "iteration: 487, cost: 299.24621990856224\n",
            "iteration: 488, cost: 299.244792152296\n",
            "iteration: 489, cost: 299.2433884075298\n",
            "iteration: 490, cost: 299.24200813248626\n",
            "iteration: 491, cost: 299.2406507976157\n",
            "iteration: 492, cost: 299.2393158853191\n",
            "iteration: 493, cost: 299.23800288968016\n",
            "iteration: 494, cost: 299.2367113162001\n",
            "iteration: 495, cost: 299.23544068154115\n",
            "iteration: 496, cost: 299.2341905132736\n",
            "iteration: 497, cost: 299.2329603496303\n",
            "iteration: 498, cost: 299.231749739266\n",
            "iteration: 499, cost: 299.23055824102175\n",
            "iteration: 500, cost: 299.2293854236954\n",
            "iteration: 501, cost: 299.2282308658165\n",
            "iteration: 502, cost: 299.2270941554269\n",
            "iteration: 503, cost: 299.2259748898663\n",
            "iteration: 504, cost: 299.22487267556187\n",
            "iteration: 505, cost: 299.2237871278234\n",
            "iteration: 506, cost: 299.2227178706431\n",
            "iteration: 507, cost: 299.2216645364989\n",
            "iteration: 508, cost: 299.2206267661642\n",
            "iteration: 509, cost: 299.219604208519\n",
            "iteration: 510, cost: 299.21859652036846\n",
            "iteration: 511, cost: 299.21760336626295\n"
          ]
        }
      ],
      "source": [
        "betas, steps, costs = gradient_descent(X, y, alpha=0.0000003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cnq33Adv1RD"
      },
      "source": [
        "If we plot the cost function $J$ against the number of iteration, we get a plot as shown below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "dd0cgF4ADmhG",
        "outputId": "433904c2-7f3b-4e8a-e675-d1506fe8f5f3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(costs)\n",
        "plt.xlabel(\"No. of iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.ylim(0, 2000)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUZ_WKyJyqev"
      },
      "source": [
        "As you can see, the cost function $J$ decreases with number of iterations and finally saturates around $299$ in $511^{th}$ iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53s4STK-wU3o"
      },
      "source": [
        "Let's see the optimal parameters found by Gradient Descent algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "1vOc-SqbwhqW",
        "outputId": "87b4f55c-91b8-4ce6-a1bf-e4b83ca11feb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<strong>TV, Radio, and Newspaper</strong> <br>\n",
              "$y$ = 1.75 + 0.05$x_{1}$ + 0.20$x_{2}$ + 0.01$x_{3}$ <br>\n",
              "$x_{1}$ = TV <br>\n",
              "$x_{2}$ = radio <br>\n",
              "$x_{3}$ = newspaper\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "message = \"\"\"<strong>TV, Radio, and Newspaper</strong> <br>\n",
        "$y$ = {:.2f} + {:.2f}$x_{{1}}$ + {:.2f}$x_{{2}}$ + {:.2f}$x_{{3}}$ <br>\n",
        "$x_{{1}}$ = TV <br>\n",
        "$x_{{2}}$ = radio <br>\n",
        "$x_{{3}}$ = newspaper\n",
        "\"\"\".format(*betas[0], *betas[1], *betas[2], *betas[3])\n",
        "display(HTML( message ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOvWRz0tGrPN"
      },
      "source": [
        "*TV, radio* and *newspaper* are the input variables, $x_1, x_2$ and $x_3$ respectively and _sales_ is the output variable, $y$. We obtain a multiple linear regression model of $y = 1.75 + 0.05x_{1} + 0.20x_{2} + 0.01x_{3}$. Note that each of the estimated model parameters (i.e., $\\beta_{0}$ through $\\beta_{3}$) have been rounded to 2 decimal places.. Intercept, $\\beta_0$ has been estimated as $1.75$ and three regression coefficients, $\\beta_1$, $\\beta_2$ and $\\beta_3$ have been estimated to $0.05$, $0.20$,  and $0.01$ respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T4RprYQ20uf"
      },
      "source": [
        "You might have noticed that the parameters found by Gradient Descent are similar but not exactly the same as that found by OLS. This is because the Gradient Descent depends on the factors such as the learning rate and the number of iterations for which it runs. Selection of these factors heavily affects the solution that the Gradient Descent finds.\n",
        "\n",
        "*You can play around with the learning rate and the number of iterations and see if the soilution matches to that of OLS*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmiCEuhOobGU"
      },
      "source": [
        "## Comparison with Ordinary Least Squares method\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA81KOHEuDeX"
      },
      "source": [
        "While both of the techniques find the parameters that minimize the cost function, their approach is quite different. As mentioned earlier, Ordinary Least Squares (OLS) is a direct method to find the optimal parameters whereas the Gradient Descent is an iterative method. This is the main difference between the two approaches. Few other points that differentiate the two approaches are:\n",
        "\n",
        "* You don't need to choose any hyperparameter in OLS whereas Gradient Descent requires you to choose a learning rate and the number of iterations.\n",
        "\n",
        "* OLS requires the number of samples $n$ to be greater than the number of features $d$. Also it requires $(\\mathbf{X}^{T}\\mathbf{X})^{-1}$ to exist. However, there are no such constraints in Gradient Descent.\n",
        "\n",
        "* If the constraints of OLS are met then it always gives the exact solution. Whereas the solution provided by the Gradient descent is dependent on the learning rate and the number of iterations and may not always be exact.\n",
        "\n",
        "* The time complexity of OLS is $O(d^3)$ whereas the time complexity of Gradient Descent is $O(kd^2)$ where $k$ is the number of iterations. So if the number of features is large (exceeding $10,000$), then its a good idea to choose Gradient Descent.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJXZe1kBoJCn"
      },
      "source": [
        "## Key Takeaways\n",
        "\n",
        "* Gradient of the cost function w.r.t each of the parameters can be derived easily using calculus.\n",
        "\n",
        "* The parameters are updated iteratively using their corresponding gradients.\n",
        "\n",
        "* When the dataset is large with a large number of features, Gradient descent is preferred instead of OLS.\n",
        "of the function.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
